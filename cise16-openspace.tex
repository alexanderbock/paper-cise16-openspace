\documentclass{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)
%\documentclass[electronic,journal]{vgtc}     % electronic version, journal

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
\usepackage[bookmarks,backref=true,linkcolor=black]{hyperref} 
\usepackage{subfigure}
\usepackage{gensymb}
%\usepackage{flushend}
\usepackage[margin=1cm]{geometry}

\def\etal{\textit{et al.}}
\setlength\fboxsep{0pt}
\hyphenation{analysis}

\newcommand{\todo}[1]{\textbf{[~}\textcolor {red}{#1}\marginpar{\textcolor {red}{\centerline{{\Huge \textbf{!}}}}}\textbf{~]}}
\newcommand{\question}[1]{\textbf{[~}\textcolor {darkgreen}{#1}\marginpar{\textcolor {darkgreen}{\centerline{{\Huge \textbf{!}}}}}\textbf{~]}}
\newcommand{\diff}[1]{\textcolor{blue}{#1}\marginpar{\textcolor{blue}{\centerline{{\Huge \textbf{!}}}}}]}
\newcommand{\summary}[1]{\textcolor{Magenta}{\noindent #1}}

\title{OpenSpace: Bringing Astronomic Discoveries to the Masses}

\author{Alexander~Bock%
\thanks{e-mail: \{alexander.bock\,$\vert$\,anders.ynnerman\}@liu.se}\\%
\scriptsize Link\"oping University%
\and Carter Emmart%
\thanks{e-mail: carter@amnh.org}\\%
\scriptsize American Museum of Natural History%
\and Anders Ynnerman$^\ast$\\%
\scriptsize Link\"oping University%
}

%% Abstract section.
\abstract{%
}

%% Uncomment below to include a teaser figure.
\teaser{
  \newcommand{\abImageHeight}{4.9cm}
  \centering
}

\begin{document}
\maketitle

Since the beginning of time, humans have looked up to the night sky in an attempt to understand the world around them. The movements of the Moon and stars and planets have shaped calendars and aided in navigation, while the shape of star patterns in the night sky have sparked the creativity in many tales all around the world.

Undoubtedly, the history of mankind has been shaped by the five planets, our moon, and 10000 or so stars that are visible to the naked eye. However, the invention of telescopes and satellites has shown us that this is only a tiny slice of the eight planets, hundreds of moons, billions of galaxies, and sextillion stars in the observable universe altogether.

Understanding the scientific discoveries and their results are key to understanding our cosmic origins, potential dangers, and the long-term future of our species. While amateur telescopes are available to everyone nowadays that allow a deeper view into the cosmos, only a small enthusiastic subset of people bother with the calibration, assembly and usage of home telescopes. The rest of humanity has to rely on professional telescopes to provide the capability to capture the light and produce stunning pictures of the night's sky that are the result of very long integration times which, in some cases, cannot even be reproduced by the human eye. However, the context in which these images are taken is lost in most cases, even when it comes to trivial tasks as to locate the photographed area on the sky. For many years, planetariums provided this context to the public by reproducing the night sky on the planetarium surface to support explanations and tell a compelling story to the audience. These explanations are often provided by an expert in the field and provide easy access to the knowledge about the cosmos and all its wonders.

These guided experiences are useful and interesting events for the hundred or so visitors that can be seated in a planetarium at the same time, but they cannot reach a broader audience due to the physical limitations of requiring a planetarium. With modern technology and the digitalization that all planetariums underwent during the last decade, it becomes possible to enhance the previous presentation methods by utilizing fully immersive 3-dimensional, stereoscopic computer graphics. This allows planetarium visitors to not only look at the night's sky, but leave Earth and view any kind of content instead of a static view of the night sky, which, in turn, enables the public dissemination of a much broader array of discoveries.

As good as these developments have been, they have not solved the issue of scalability. One partial solution that has been employed is the ability to connect multiple planetariums for a shared one-to-many experience. In these cases, a single expert is presenting the information not only to a single planetarium, but his voice and video is transmitted to other, connected, planetariums to reach a broader audience. This solution, however, limits the distribution of these lectures and explanations to other planetariums of a bigger size, due to cost-factors that inhibit smaller venues to purchase the same specialized software to participate in these events.

By providing the rendering software for free as open-source, it is possible to reach a much broader audience for these educational events. In addition, by designing the software in such a way that it can produce full-fidelity renderings on both planetarium surfaces as well as regular desktop or laptop monitors, the outreach can be exponentially bigger as a much larger audience can experience and participate from their home computers. While this removes the immersive experience of the explanations, it opens up the public access by an order of magnitude or more. We foresee, however, an upcoming change for the lack of immersion. The ongoing proliferation of virtual reality headsets, such as the Oculus Rift, make it possible to experience the same immersive experience on the home computer as is felt in a planetarium setting. And with the prices of these devices falling in the upcoming years, many more people will be able to afford and use such devices in an interconnected setting as described above.

The second evolutionary step is the gradual transition from \emph{what} has been discovered by telescopes and measurements over to \emph{how} it is being discovered. The idea behind this is to not only show the scientific results spacecraft's mission, but also explain the intricate process of the data acquisition itself. By making the general public part of the scientific discovery, it allows them to understand the underlying complexity behind the mission and get a deeper appreciation for the collected data. This will ultimately result in a better educated populous that is more willing to spend the effort and resources on additional missions to widen our scientific horizon.

The rest of this article is organized as follows: First, we will describe the basic conceptual ideas behind the open-source software \emph{OpenSpace}, which can be used for these public disseminations before focussing on individual aspects of the software; the available data structures and data sources that and the domecasting ability to connect multiple software instances. Then, we visit the concept of science dissemination in greater detail with three focus topics, the \emph{New Horizons} and \emph{Rosetta} space missions as well as the prediction and research of space weather.

\section*{Software}
\todo{Why prefer interactive shows vs prerendered?}
In order to facilitate the mass dissemination of astrophysical and astronomical data, we developed an extensible open-source software called \emph{OpenSpace} that can be used as a platform to display various kinds of data. The system is not limited to any specific scientific domain, but has been developed to support a large variety of contextualized visualizations, such as models, volumetric data, data-derived geometry, abstract visualizations, and more. By providing a rich modular framework for individual developers, it is easily possible to integrate new visualizations and keep the same look-and-feel and contextualized information that other people have contributed to the software as a whole, thus building an ecosystem of modules that can be used by anyone.

Many systems have previously provided such a service, such as Uniview \cite{XX}, Digital Sky, Digistar, or Eyes on the Solar System, to name a few. Many of these systems are either vendor-specific or commercial products, which naturally limits the possible reach of those products. In addition to requiring the purchase of specialized hardware or licenses, which puts a severe limitation on smaller venues that cannot generate enough revenue to allow for such an expense, the closed-source approach of these products also severely limits the variety of data that is displayed. In these systems, content creators are limited to the rendering capabilities that are included and exposed in those softwares and are, usually, unable to write and integrate their own rendering modules that introduce completely novel visualization techniques. While this capabilities tradeoff allows for a more stable codebase and superior end-user experience, it also limits the integration of new features, especially for venues that cannot afford to commission a custom rendering module written by the vendor directly.

The different approach to this tradeoff that we have taken is the use of open-source software, reducing the necessary investment to develop and include new rendering techniques by exposing the entire software for customization. One important historical software is Celestia, which chose the same approach and was successful, but the open-source development effectively stopped in 2010. With OpenSpace we are trying to build on the success of the open nature of open-source software to provide a content-rich, community-curated environment for the general public as well as specialized planetariums. Having a central instance that combines the efforts of small-to-medium venues and private citizens makes it possible to combine the benefits of a wide user base with the necessary expert knowledge for data integration. In the following section, we will elaborate on the building blocks that are used to generate these modules.

\section*{Data Sources}
The best visualization is useless if there is no data to visualize. Furthermore, it is absolutely necessary to display the available information scientifically accurate as to not mislead target audience of the visualization. In this section, we are describing some of the techniques and data set sources used in \emph{OpenSpace} in order to accurately depict the available data and provide the necessary context to it.

\subsection*{ScaleGraph}
Integrating many datasets into a common frame of reference in an interactive setting can be taxing, the large-scale aspect of astronomical data provides an additional challenge as the floating point representation of numbers in computers does not have infinite precision. This poses severe restrictions of the relative scales of objects which can be represented in a single scene. The most widely used floating point number formats in computing are defined in IEEE 754 as single and double precision with 32 bit and 64 bit respectively. Combining the floating decimal point and the finite storage means that the precision to represent numbers decreases with increasing magnitudes. This does not become an issue as long as objects of interest of similar magnitude are displayed, as a linear scaling factor can always map value ranges into a more favorable distribution around 0. However, if objects of vastly different magnitudes are involved, this uniform scaling is no longer applicable and has to be replaced with a more elaborate method.

As an example, the minimal value which can be added to 0 for single precision numbers is $\approx 10^-7$ while the minimal number that can be added to $10^10$ is 1000. Every number smaller than this will be rounded to the same result. This also leads to the paradoxical situation that \texttt{while (f + 1 != f)} does not result in an infinite loop, but does always terminate.

Without any special consideration, this inhibits the use of a single coordinate system for objects even in our solar system. If the Sun is the origin of the coordinate system, the floating point precision error at Pluto is so great, that the whole planet only contains 8 distinct distance values, with all intermediate values rounded to these distinct values \todo{Add picture?}.

One commonly used approach to solve this issue of including objects of various magnitudes in the same view, which we also employ, is the concept of a ScaleGraph. We refer the reader to a paper by Klashed \emph et al.~\cite{Uniview} for details on this. In essence, the scene is described a tree that contains different scales as nodes. Each scale describes distances to child nodes, and objects in the scale in its natural magnitude, utilizing the above mentioned scaling. This graph structure then enables to translate and scale each visible object in the scene to be represented at the correct relative size depending on the location of the camera in this scale graph without the visible loss of precision. This, in essence, provides a scaling for all objects in the scene that is purely dependent on their connection within the scale graph. While this solution introduces its own issues when it comes to stereoscopic rendering and scale transitions, it has proven to be a very successful solution to the scale problem and superior to other approaches.

\subsection*{SPICE}
In order to render astronomical objects in their correct positions and provide the ability to animate them through time, potentially complex trajectories have to be computed for all objects. In the easiest case of perfect orbits, the only required information are the six Keplerian elements, eccentricity, semimajor axis, inclination, longitude of the ascending node, argument of periapsis and mean anomaly, from which the absolute location of a body can be retrieved relative to a target reference frame. However, this simple representation does not capture any complex orbital mechanics such as the influence of additional bodies or effects from general relativity such as seen in the precession of Mercury. Therefore, a more sophisticated method for determining complex orbits has to be employed.

One widely used library called SPICE is provided by NASA that provides mission planning quality access to location and attitude of spacecraft, planets, and other objects through the use of so-called \emph{kernels}. One type of SPICE kernels provides location information for one or more spacecraft for a specific time period. While these can be provided analytically, through the use of Keplerian elements, in most cases a higher order approximation or explicit sampling is used to capture, for example, the precession of planets. These approximations or explicit specification of locations are especially necessary for spacecraft with a complex trajectory, such as Rosetta or New Horizons. Using the SPICE library provides instant access to all of this information and the capability to query locations in a variety of coordinate systems, allowing the use of the best coordinate system for each object.

While the main use case of SPICE is the ability to query location and attitude, it also provides many more features which are useful for visualization frameworks. For instance, the library allows direct access to instruments onboard spacecrafts and allows querying information about these instruments, such as the viewing angle, the shape of the field-of-view, or other information such as conversions regarding the on-board mission clock. Furthermore, it supports the simulating the light travel time between objects, which can be important when measurements of objects at large distance are to be matched.

In fact, SPICE kernels are heavily used in the planning phase of a mission, which has the additional benefit of making these kernels readily available for a large variety of missions and also guarantees the accuracy of these kernels. While the scientific accuracy during the visualization has to be sufficiently high, the spacecraft planning and development phase has an even higher level of accuracy.

\subsection*{Digital Universe}
\todo{Reference A Flight through the Universe CISE}
While most bodies in the solar system can be covered by using SPICE kernels, it is unfeasible to provide SPICE kernels for every star in the milky way and for each galaxy in the universe. Measurements of stars and galaxies have been performed by many different groups over many years, leading to many inconsistencies both in the collected data as well as the data formats.

One successful effort to curate all the available datasets is the Digital Universe, as published by the American Museum of Natural History \cite{DigitalUniverse}. It collates all of the information, such as position, color, imagery, spectral measurements, and much more about visible stars in the milky way, as well as information about visible galaxies, quasars, and other measurements. By incorporating this database, it is possible to render a visually appealing and accurate representation of the night's sky to provide necessary context to activities in the solar system, let alone the possibility to integrate visualizations of events outside of the solar system.

The rendering of the stars is a particular aspect that is very error-prone. The stars are rendered as geometry shader generated camera-aligned billboards where the billboard is resized both according to the distance to the camera as well as the absolute brightness of the star; the closer and brighter a star is, the bigger the billboard becomes. This represents an artistic choice. In the idealized rendering, each star, regardless of distance and brightness would have the same size, which would be very close to a perfect point light source (Sirius, the brightest star, has an angular extent of $8.5 \cdot 10^{-07}$ degrees from Earth). However, due to optical effects either in our eyes or telescopes, this point light source is subject to a point spread function creating an extended image. The result is depends on the apparent magnitude of the star, which, in turn, is dependent on the distance to the camera and the absolute brightness of the star. We simulate the point spread function of a perfect telescope on the billboard by a gaussian falloff. This results in a realistic and appealing visual representation (see Figure XX). By computing the apparent magnitude on-the-fly, we also account for the changing location of the camera when moving outside of the solar system. This allows all stars to be rendered in a way in which a telescope at the position of the virtual camera would perceive them. In addition, by exchanging the texture which is used for the point spread function, it is possible to simulate the visual appearance from different telescopes.

The same billboarded approach can be applied to all galaxies except the milky way. Instead of using a gaussian point spread function, the correct image as captured by telescopes is used instead with the billboard resized to reflect their accurate projected size regarding their location and distance to the camera. These images can either be reconstructed from optical light or any other detection method.

While inside our solar system, the Milky Way cannot be projected as a billboard as it surrounds camera in every direction. Hence we make use of a textured sphere that surrounds the solar system, which has an omnidirectional texture of the Milky Way mapped to its inside surface. For the texture we use a modified version of an all-sky survey \todo{correct?} by Mellinger \cite{Mellinger}. A post-processing is applied to this image to remove all of the stars that are visible such that only the nebula \todo{check with Carter on this} parts and the galactic dust of the galaxy remain. While this representation looks correct while the camera is in the solar system, the sphere has to be replaced by a simulated, volumetric representation of the Milky Way when the camera is outside the local area of the solar system and into intergalactic space. This volumetric data is generated from statistical models of star distribution in our solar system, which has been used to simulate the presence of various kinds of dust, the location of stars and much more. \todo{Double check with Jon Parker, how about rotation?} 

\subsection*{Models \& Planets}
Apart from supporting the rendering of models using a large variety of standardized file formats, such as OBJ waveform, planetary surfaces are an important aspect impacting the visual fidelity of the system. For many planets in the solar system, such as Earth, Mercury, and Mars, humanity has acquired images of such high detail that simple straightforward texturing is unfeasible. Thus, a hierarchical level-of-detail approach is necessary that takes the distance of the camera to the planet, as well as the footprint of the planet on the display, into account. The same aspect applies to detailed terrain models, such as heightmaps, which are subsequently used to position other objects on the surface of a planet.

High resolution planetary images and heightmaps are usually distributed in a quad-tree level-of-detail approach. One of the most widely used protocols are the standardized Web Map Services (WMS), for which NASA's Global Image Browsing System (GIBS) is an example. These are web services where clients can request images for a specific region, which are delivered as images in a standardized format. It is up to the client software to then correctly align and display these images and interpolate between different detail levels. An additional requirement is to be able to layer different maps to highlight interesting features. For instance on Earth, there are maps of high resolution images of the surface, which have been collected over a large amount of time such that covering clouds can be removed, revealing the ground underneath for all locations. This map can be augmented with an additional layer showing only the current clouds without the background. Using this, it is possible to achieve a high resolution surface image with up-to-date cloud imagery, too.

Supporting a wide variety of standards for image loading and localization is an important part for specialization as many venues have access to higher detailed arial photography of their surroundings and need the ability to integrate this into the scene to provide a higher fidelity experience for visitors. Venues can make use of these techniques to integrate high resolution aerial photography of their local neighborhood to provide interesting tailor-made content. A database of these higher terrain resolutions could be a possible citizen scientist project enabled by our framework. Another example for this is the use of Mars Reconnaissance Orbiter's HiRise images of Mars, which have a sub-meter per pixel resolution, but are limited to a select few locations. Integrating this, and deriving higher resolution heightmaps from these images, is a worthwhile citizen science goal.

\subsection*{Image Projection}
Looking at the fleet of space craft in the solar system, there are many available modalities for the available instruments, such as radiometer, dust counter, magnetometer, ... . However, one of the instruments that is most intuitive for layman are image-generating cameras. From the cameras onboard the Luna 3 craft taking a picture of the far side of the moon in 1960, up to the modern cameras onboard the New Horizons space craft, images have always been inspiring the public and were hugely successful in generating public interest and understanding.

It is due to that reason that we focussed on the display of images first. The goal was to not only present the final image without context to the public, but also show the context of the acquisition time and the spatial relationship to the target body and other images taken. In order to achieve this, we make use of an image projection method \cite{ImageProjection} to project the images taken onto the target body. Using the camera's field-of-view, acquisition time, and pointing direction, it is possible to treat the camera as a virtual projector which casts the image onto the target body (see Figure~XX). This method automatically takes care of projection artifacts and is easily generalized to both multiple images as well as non-square image geometries. Furthermore, by including an additional virtual image plane which can be projected upon, it is possible to handle images in which the target object is filling the entire frame, or the subject of interest is on a physical body represented in the scene all together. This technique can be used to, for instance, capture the solar wind streaming out from the Sun, or the outgassing of material from \todo{moon} (see Figure XX).

Additional complications present themselves when generalizing the image projection onto concave objects. While it is trivially applicable for spherical bodies as there is no self-shadowing on those bodies, it becomes more difficult on complex geometries. The standard image projection algorithm does not include any occlusion checks, which can lead to undesirable visual results (see Figure XX). The reason is that each location on the object is checked against the projected image independently, not considering other objects that may lie between the current location and the projection source. The solution to this problem lies in applying a raycasting type approach. By constructing a ray between each potential target location and the projection source, this ray can be checked against the geometry of the object to detect intersections and thus shadowing. If there is an intersection, then a previous point would be the correct point to project onto instead. While raytracing is usually a prohibitively expensive operation in real-time rendering, in this case it is admissible as it only has to be performed once for each image that is projected. With this method it is possible to project images not only onto regular bodies such as planets and moons, but also irregular bodies such as comets.

\section*{Domecasting}
One important feature of \emph{OpenSpace} that enables the mass public dissemination of scientific topics is the concept of \emph{Domecasting}. Using a set of features, this allows multiple instances of \emph{OpenSpace} to be cross-linked and replicate the same renderings on a multitude of distributed systems. The \emph{master} node is in control of all connected \emph{slave} nodes and every interaction that happens on the master node is replicated to the slaves, keeping them synchronized. Each instance of \emph{OpenSpace} is running independently, but rendering settings (camera position, simulation time, rendering visual settings, etc) are synchronized over a network connection from the master to the client. Any small delay in his communication is acceptable as long as it stays well below a second. For a truly collaborative experience, it is also desirable to be able to change the which instance is designated as the \emph{master}. This enables a ping-ponging between presenters that are physically separated and enables common decision making or information dissemination to a public.

In order to make the system more scalable, the network messages are not sent from the master to the slaves directly, but are routed through a server, which in turn distributes the messages to the slaves. On the one hand, this allows the creation of a priority list in the case that the outgoing network bandwidth is reached, making it possible to specify a set of $\alpha$ sites which will always be served first, with a set of $\beta$ and $\gamma$ sites in descending priority. On the other hand, it is the basis for load balancing, by introducing intermediate servers which simultaneously act as slave nodes to the main server and master to their own list of slaves. Furthermore, in many cases, the master node might be located in an area with severely limited outgoing bandwidth, such that the capacity would be quickly exceeded if multiple slaves were to connect. By placing the main server in a stable environment with a high-speed internet connection, we also allow the controlling of domecasting experiences from remote locations.

Each \emph{OpenSpace} instance runs independently, instead of receiving a prerendered image, in order to support the local rendering geometry. For example, some slaves might run a multi-pipe planetarium setup while others run a single machine flat screen. By abstracting this away from any remote operations, the system becomes more stable and agnostic to rendering changes.

As mentioned before, the most important settings to be synchronized are the virtual camera information, the simulation time, as well as the change in simulation time (in simulation seconds per realtime second). Additional information are all settings which influence some aspect of the rendering, such as textures, adjustable transparencies, and others. Additionally, recorded audio and video needs to be transferred to all slaves which can then be displayed locally.

\section*{Disseminations}
Any kind of publicly funded scientific discovery has to be disseminated back into the public. In the case of astronomical discoveries, expensive and labor-intensive technology has to be utilized to achieve new discoveries, which naturally incurs a high cost. In order to justify these expenses, and garner continued future interest in these discoveries, the public has to know that they are participating and benefitting from these discoveries, either directly or vicariously. In the case of planetary missions, it is the increase in general knowledge of humanity that is desirable, as well as the discoveries that provide information about the creation of the solar system in addition to the more tangible effects of technological discovery.

Humans have an innate interest for the vastness of space and are eager to learn about places most people will never be able to physically see with their own eyes. Thus it is important to make these locations feel as real as possible to foster and nurture that innate curiosity.

One of the most successful methods of teaching, that is as old as humanity itself, comes in the form of a teacher-learner relationship. All the way from the antique until the beginning of the 20th century, this relationship has stayed fundamentally the same, there is one teacher interacting with learners which are all located in the same physical space. This tradition started to be broken by the use of radio shows and television, which provided the teachers with access to a large number of learners, which was not physically possible before. This, for the first time, also enabled the general audience to participate as learners, which previously was not possible as learners had to physically fit in the learning space and travel there.

Today, the use of e-learning material and free-for-all study courses provided on the internet has surged this form of learning. Any person with an internet connection can achieve a higher learning goal today than the wealthiest person just 100 years ago.

By creating a suitable software that supports this distributed learning by providing the context of the information that is transmitted, we can support learners throughout the world in understanding new concepts and ideas. For the best results, this knowledge transfer requires the presence of a teacher that is an expert in the field.

In the rest of this section, we will highlight three different events, that we have been focussed on; 1. the \emph{New Horizons} space craft performed a fly-by event at Pluto in July 2015 during which much information about this previously unvisited dwarf planet was collected; 2. The \emph{Rosetta} spacecraft entered the orbit of the comet 67P/Churyumov-Gerasimenko in August 2014 and took measurements of the comet during its approach to the Sun, which increased the comets activity; 3. The study of the solar system conditions collectively known as \emph{Space weather}. In most cases, this refers to the conditions of the plasma being ejected from the Sun both as a continuous stream, the solar wind, as well as abrupt eruptions, such as coronal mass ejections. All these events had different parameters and objectives and provided a variety of challenges, whose solutions can now be applied to similar missions, such as Juno.

\subsection*{New Horizons}
NASA's mission flew by the Pluto system on July 14th, 2015 and took measurements with its seven instruments. Of special interest are the LORRI and RALPH instruments, that provided images of Pluto's its moons' surfaces, as well as REX that took radio measurements of Pluto's atmosphere. In our system, the images are projected using the method described above and the REX occultation measurements are represented by a line connecting the space craft and Earth. The measurement times for all instruments are presented to the user, but not all instruments have a direct visual mapping, for example the SWAP instrument measures solar wind density values. This mission was disseminated to the larger public during a public, global event in which 13 different locations throughout the world participated. During a 2h live show, which coincided with New Horizons closest approach to Pluto, experts on the mission team explained details of the desired outcome, using OpenSpace as the source of the contextualization for these informations. In addition to the live audience in the participating locations, a video stream of the event was available on the Internet, which was also later provided as a video-on-demand, called "Breakfast at Pluto".

\subsection*{Rosetta}
After the arrival of ESA's mission at the comet 67P/Churyumov-Gerasimenko, it has since been orbiting the comet at various altitudes, providing measurements about comets activity, its mass, and the amount of outgassing. Among the instruments that we utilized sofar are the fairly low resolution NAVCAM, which is primarily used for navigational purposes, but its images were released to the public much earlier than from other instruments. The OSIRIS camera is a high-resolution 2048$\times$2048 pixel optical camera with a narrow angle and wide angle lens. Images from both instruments can be used with the same image projection technique as described above, utilizing the addition of the image projection technique for concave objects.  

\subsection*{Space Weather}
\begin{enumerate}
\item Bringing the scientist tools (iSWA) into planetariums
\item NASA's Living with a Star program
\end{enumerate}



\bibliographystyle{abbrv}
%%use following if all content of bibtex file should be shown
%\nocite{*}
\bibliography{bibliography}
\end{document}

